# Transformer - Naver Boostcamp 13주차 개인 보충학습 

## Encoder
- 모든 단어 인풋을 **동시에** 처리 가능. 

Self-Attention -> Feed Forward NN의 구조를 가지며, 여기에서 Self-Attention이 중요한 구조이다.

n개 단어가 주어질때, 그중 하나를 encode할때 모든 다른 단어도 같이 참여하는 것이다. 이 과정은 문장 속에서의 단어들의 관계를 학습하기 위해 결정적인 요인이다.

인풋 단어 당, Query, Key, Value 벡터를 각각 생성한다. 그리고 기준 단어 하나를 인코딩할때, 그 단어의 Query와 나머지 단어의 Key 쌍을 모두 내적하여 Score 벡터를 만든다.

Q*K 값은 sqrt(key_dim) 으로 나누고 softmax 층을 거쳐 normalize된다. 이것을 최종적으로 Value 벡터를 이용해 가중합 시키면 기준 단어의 인코딩 벡터가 된다. 

Query와 Key는 같은 차원이어야 하지만 Value와는 가중합을 하므로 차원이 같을 필요는 없다.


## Positional Encoding
