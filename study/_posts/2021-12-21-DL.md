---
layout: post

---

# 딥러닝 관련 개념 정리 - 3

> 딥러닝 관련 주요 개념을 정리합니다.
>
> 

## 가중치 초기화

- 균등분포
- 정규분포



## Xavier

sigmoid 쓸때 많이 쓰던 방식인데, 가중치 소실 문제가 생긴다.

입력 분산과 출력 분산이 많이 달랐어서 sigmoid 로 인한 그것을 해결하기 위해 Xavier 초기화 기법 도입.

하지만 이 방법은 ReLU에서는 적용이 잘 되지 않았고 He 초기화가 적용이 잘 된다.

He 초기화

입력값의 분산을 늘려서 출력값의 분산을 0,1에만 치중되지 않도록 한다.

학습률 최적화

Warm up , SGD + Scheduling (lr을 어떻게 조정할지 미리 짜두는것)

논문에 있는 scheduling 방법들을 적용해 보는 게 좋다.

딥러닝에서 hyperparameter search 하는 방법?

- 일단 머신러닝보다는 시간적/현실적한계가 크다.
- 최대한 하이퍼파라미터를 줄이자. 현실적으로 조정하기 힘들다.
- 선례들을 참고해서 반영하는게 시간적으로 현실적으로 편함.

Weight decay

가중치 크기가 학습에 따라 커지게 되는데 (과적합), 이를 방지하게끔 직접 규제. 매우 강력한 규제이다. 실제 모델의 성능을 하락시킬 위험도 존재함.

과적합 방지 방법

1. 데이터 차원 - Data Augmentation

2. Loss function - weight Decay

3. Model 에 제약을 거는 방법

4. 1.  Dropout

      \- 앙상블 효과 (아예 다른 모델을 합쳐서 앙상블하는 듯한 효과를 보임)

   2. Batch Normalization

      보통 레이어 > 활성화함수 > BatchNorm > 레이어 이런식으로 많이 들어감. Dropout 과의 호환성을 위해 해당 위치에 주로 들어간다.

   3. Dropout과 BN 은 동시에 쓰기 힘들다!

5. 학습 방법 - Early Stopping / K-fold / 앙상블

Early Stopping

검증손실을 모니터링하여 예를 들어 5번 이상 val loss 가 감소하지 않으면 학습 중단시킴

문제점 : 하이퍼파라미터의 증가. stopping을 얼마나 허용해야 할지 직접 지정해줘야 하므로.

ModelCheckpoint -> save_best_only = True 로 설정하면 best model 만 가져감.

Sequential

단점 : 입력도1개, 출력도1개

입력이 2개 따로, 출력도 중간 출력 1, 마지막 출력 1 이런식으로 하려면? -> **함수형 API**

함수형 API

함수형 API를 이용하면, 내맘대로 input / output 위치나 개수 등을 조절 가능하다. 자유롭게 활용 가능.

서브클래싱 API

아예 나만의 layer, dense, 모델을 만들 수 있다.