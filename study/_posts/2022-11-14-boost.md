# Deep Knowledge Tracing (DKT) - Naver Boostcamp 9주차

## 분류에서의 Confusion Matrix
Area Under the ROC Curve (AUC) 
- x축은 FPR, y축은 TPR
- 밑부분 면적이 넓을 수록 모형 성능이 좋아진다.
- "맞으면 위로 올라가고, 틀리면 오른쪽으로 간다" 로 이해하면 편하다.

AUC가 좋은 2가지 이유 (이상적)
    1. 척도 불변.
    2. 분류 임계값(proba) 불변

그런데 척도 불변, Proba 불변이 항상 이상적이진 않다.
- 잘 보정된 확률 결과가 필요한 경우가 있는데 AUC로는 이 정보를 알 수 없다.  **질문: 말뜻?**
- FN, FP 비용에 큰 차이가 있는 경우, 비용이 큰 것을 우선시하고자 할때 AUC는 최적화에 유용한 방법이 아닐 수 있다. (스팸 메일 감지)
- Imbalanced Data에서 AUC가 높게 측정되는 경향이 있다. 그래도 모델끼리 상대적 성능은 동일한 test data 하에 비교 가능.

## DKT History 및 Trend
DKT는 Sequence Data를 다루는 만큼 자연어 처리에 많은 영향을 받아왔다.
![](../images/dkt_trend.png)
### Sequence Data
RNN
- 장문장에서 학습이 어려움. 거리가 먼 단어와의 관계 정보가 소실됨.

LSTM
- 장기기억 저장

SEQ2SEQ
- Encoder and Decoder
- 그 사이에 **Context Vector** 존재

LSTM, SEQ2SEQ도 결국 아주 긴 문장에 대해서는 한계가 있음

Attention
- 어텐션 메커니즘을 더해줌.
- Seq 방식의 한계가 여전함. 학습속도가 느림.

Transformer
- 병렬 처리 방식을 적용. 
- 그럼 말의 어순정보는? **Positional Encoding** 으로 해결함!
- \+ Self-attention

크게 Sequence 모델링에는, 
1. LGBM
2. LSTM
3. 1D-CNN
4. Transformer

이렇게 쓰일 수 있다고 알아두자.
