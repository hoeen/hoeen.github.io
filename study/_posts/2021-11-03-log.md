---
layout: post
categories: [TIL]

---

## Today I Learned - 211103

### Feature importances

cardinality가 높은 경우 feature importance가 높아질 수 있는 문제가 있다.

drop-column importance : 한 feature씩 없애서 학습을 하는 경우. 노가다. 정확하게 볼수있겠지만 시간이 많이 걸림

permutation importance : 기본 특성 중요도와 Drop-column 중요도 중간에 위치하는 특징. 무작위로 특성에 노이즈를 준다. 

중요도가 -인 특성은 제거해도 모델 학습에 영향을 별로 미치지 않는다.

Boosting

주의점 : randomforest보다 하이퍼파라미터에 민감함

잘못 관측한 데이터를 다음 모델에서 더욱 신경쓰는 방식으로 반복한다.

Gradient Boosting

Adaboost에서는 샘플들의 가중치를 조절하면서 학습을 진행하였다면, Gredient Boosting 에서는 **잔차** 를 학습하도록 한다. 

참고 : monotonic constraints - 단조 상관관계가 있는 경우 오류가 생기지 않도록 도와주는 옵션이다.

early stopping을 사용하여 과적합을 피함

방위 데이터 : 범주형 중 명목형 데이터일듯. 하지만 여러 실험이 필요하다. 