# 네이버 부스트캠프

Item2Vec and ANN



## 1. Word2Vec
- 임베딩
	+ 주어진 데이터를 낮은 차원의 벡터로 만들어서 표현하는 방법. (차원 감소 기법)
	+ Sparse Representation : 아이템의 전체 가짓수와 차원의 수가 동일
		* one-hot encoding
		* 아이템 개수가 많아질수록 차원이 커지고 공간 낭비
	+ Dense Representation : 아이템의 가짓수보다 훨씬 작은 차원으로 표현 
- 워드 임베딩
	+ 텍스트 분석을 위해 단어를 벡터로 표현 
	+ one-hot encoding -> dense representation
	
- Word2Vec
	+ 뉴럴넷 기반 
	+ Word2Vec 학습 방법
		1. CBOW : Continuous Bag of Words
			- 주변에 있는 단어를 가지고 중앙 단어를 예측하는 방법 
			- 앞뒤로 n개 총 2n개의 주변 단어를 활용.

			- 학습 파라미터 - $$W, W'$$ 2가지
				1. $$V$$ : 단어의 총 개수, $$M$$ : 임베딩 벡터의 사이즈  
				학습 파라미터: $$W_{V \times M}, W'_{M \times V}$$ 
			- 각각의 단어가 Embedding matrix를 통해 lookup table이 되고, 이들의 평균이 Projection layer이 됨. 
			- 실제 단어와 예측한 데이터가 비교되어야 하기 때문에 원래 원핫벡터의 크기와 동일한 벡터로 반환되고 이는 softmax 층을 통과하여 각 클래스별 확률 벡터로 반환. 이가 실제 단어 벡터와 비교됨. 
		2. Skip-Gram
			- CBOW 의 **입력층과 출력층이 반대**로 구성. 벡터의 평균을 구하는 과정이 없음. 
			- 한 단어를 가지고 주변 단어를 예측하는 방식.
			- 일반적으로 CBOW보다 Skip-gram의 임베딩 표현력이 더 좋음 
			
		3. Skip-Gram with Negative Sampling (SGNS)
		![img](../images/skip_gram_SGNS.png)

			-입력값과 레이블을 모두 입력1, 2로 주고 레이블을 0, 1의 binary 로 바꾸어서 1로 예측하도록 문제를 바꿈. 
			- 그런데 레이블은 모두 1이 될수밖에 없음. 주변 단어들을 사용했기 때문
			- 주변에 있지 않은 단어들을 강제로 샘플링해서 가져와서 레이블을 0으로 함. (Negative sampling)
			- positive sample 하나당 k개의 negative sampling
				+ 이 k는 데이터가 적은 경우 5-20, 충분히 큰 경우 2-5가 적당하다 (- 논문)
			- 중심 단어와 주변 단어가 각각 임베딩 파라미터를 따로 가짐 (다른 변환표!)
			- 학습 과정
				1. 중심 단어를 주변단어들과 내적한 각각 내적값을 sigmoid를 통해 0, 1로 분류 
				--> 왜 내적? 코사인 유사도? 
				2. cross-entropy로 loss를 구하고, 이를 역전파를 통해 임베딩 matrix 업데이트 
				3. 최종 생성된 워드 임베딩은 2개이므로 선택적으로 하나만 사용하거나 평균을 사용 (성능차이가 크진 않다.)
			- **학습의 결과로 생성된 워드 임베딩 matrix를 이용하여, 다음 task에 활용하는 것이 일반적인 방법이다.**
			

## 2. Item2Vec
- 단어가 아닌 추천 아이템을 Word2Vec으로 임베딩
	+ 기존 MF도 유저와 아이템을 임베딩하는 방법임.
	+ 유저가 소비한 아이템 리스트를 문장으로, 아이템을 단어로 가정하여 Word2Vec 사용
		* 유저-아이템 관계를 사용하지 않기 때문에 유저 식별 없이 데이터 생성 가능 
	+ SGNS 기반의 Word2Vec을 사용하여 아이템을 벡터화하는 것이 최종 목표 
		* MF기반 IBCF보다 더 높은 성능과 양질의 추천 아이템 제공 
- 유저 혹은 세션 별로 소비한 아이템 집합을 생성
	+ 공간적/시간적 정보는 사라짐
	+ 집합 안의 아이템들은 서로 유사하다고 가정 
	+ Skip-gram 과 달리 모든 단어쌍을 사용
- Item2Vec 활용 사례
	+ 아프리카 TV의 Live2Vec
		* [문장] 유저의 시청 이력 [단어] 라이브 방송 
	+ Spotify의 Song2Vec
		* [문장] 유저의 플레이리스트 [단어] 노래
	+ Criteo의 Meta-Prod2Vec
		* [문장] 유저의 쇼핑 세션 [단어] 상품
	
## 3. Approximate Nearest Neighbor
- Nearest Neighbor (NN)
	+ Vector Space Model에서 내가 원하는 Query vector와 가장 유사한 Vector을 찾는 알고리즘 
	
- Brute Force KNN
	+ NN을 정확하게 구하기 위해서는 나머지 모든 Vector와의 유사도 비교를 수행해야 함. 
	+ Vector의 차원과 개수에 비례해서 연산이 늘어남.

- ANNOY : spotify에서 개발한 tree-based ANN 기법 
	+ 주어진 벡터를 여러 subset으로 나누어 tree 형태의 자료 구조로 구성하고 이를 활용하여 탐색. 
	![image](../images/annoy.png)
	- ANNOY의 문제점 : 가장 근접한 점이 다른 지역(node)에 있을 경우 후보 subset에 그 점이 포함되지 못함.
		+ 해결 방안 : priority queue를 이용하여 가까운 다른 노드 탐색 / binary tree를 여러 개 생성하여 병렬적 탐색 - 앙상블 기법 
	- ANNOY 파라미터 
		1. number_of_trees : 생성하는 binary tree의 개수
		2. search_k: NN을 구할 때 탐색하는 node의 개수 
	- 요약 :
		+ Search Index 생성하는 것이 다른 ANN 기법에 비해 간단하고 가벼움. but GPU 연산 지원하지 않음. 작은 데이터의 수는 ANNOY로 사용해 보는 경우가 많다. 
		+ Search 해야할 이웃의 개수를 알고리즘에서 보장함. 
		+ 파라미터 조정 통해 accuracy / speed trade-off 조정 가능 
		+ 단, 기존 성된 트리에 새로운 데이터를 추가할 수 없음. (트리를 다시 만들어야 하니까)
		

- 질문 리스트 
	+ KNN 과 K-means clustering의 차이 ? 
  