---
layout: post
title: Today I Learned - 20211025
categories: [TIL]

---

### Today I Learned - 20211025

### Decision Tree

지니 불순도 (Gini Impurity)

Decision Tree 는 Gini Impurity를 이용하여 값이 낮은 경우부터 뿌리부터 시작하여 아래로 내려가면서 카테고리를 나눈다.

Gini impurity = 1 - (YES probability) **2 - (NO probability) ** 2

numerical value일 때는 Gini Impurity를 어떻게 계산할까?

모든 값들(행) 의 중간값을 가지고 데이터를 나누었을 때 어느 중간값에서 impurity 가 가장 낮을 것이고 그것을 node로 쓴다.

Color choice 와 같은 경우에는 모든 조합을 고려하여 YES/NO로 Gini Impurity를 계산한다.

Gini Impurity 이외에도 불순도를 측정하기 위해 Entropy 를 계산할 수 있다. Entropy로 나누는 것은 좀더 균형잡힌 트리를 만든다는 장점이 있지만, Gini Impurity가 계산이 빠르므로 Entropy 보다 많이 이용된다.

Decision Tree 를 이용하면 굳이 파라미터들을 스케일링 할 필요가 없다.

트리는 복잡하면 과적합이 잘 일어난다. 트리의 복잡도를 줄이기 위해 자주 사용하는 하이퍼파라미터들은 다음과 같다. 복잡도를 낮추어 주면 좀더 일반적인 모델이 형성된다. 

- min_samples_split - 최소한 몇개 이상이어야 다시 분기를 나누는지 설정
- min_samples_leaf - 말단 최소 잎 갯수
- max_depth - 트리 깊이

결정트리모델은 선형모델과 달리, 비선형 비단조 특성상호작용 특징을 가지고 있는 데이터 분석에 용이하다. 특성간의 상호작용에 크게 좌우되지 않기 때문이다.